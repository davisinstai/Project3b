{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3b: Goals and Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals of this assignment are:\n",
    "* To analyze corpora, especially focusing on sentences.\n",
    "* To learn more about NLP.\n",
    "\n",
    "Here are the steps you should do to successfully complete this project:\n",
    "1. From moodle, accept the assignment. Open and set up a code space (install a python kernel and select it).\n",
    "2. Complete the notebook and commit it to Github. Make sure to answer all questions, and to commit the notebook in a \"run\" state!\n",
    "3. I wrote the comments; you write the code! Complete and run `spacy_on_corpus.py` following the instructions in this notebook.\n",
    "4. Edit the README.md file. Provide your name, your class year, links to/descriptions of any extensions and a list of resources. \n",
    "5. Commit your code often. We will take the last commit before the deadline as your submission of the project.\n",
    "\n",
    "Possible extensions (from least points to most points):\n",
    "* Augment the function `render_doc_statistics` so that it also includes some sentence and noun chunk statistics.\n",
    "* Look at the displaCy docs (https://spacy.io/api/top-level#displacy_options). Make the visualizations of the dependency parses more elegant or functional.\n",
    "* Allow the user to choose an individual document. Save all the visualizations of the sentence parses into a file.\n",
    "* Your other ideas are welcome! If you'd like to discuss one with Dr Stent, feel free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Our Packages\n",
    "\n",
    "On the command line (in the terminal), type:\n",
    "\n",
    "% `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Our Data\n",
    "\n",
    "From Moodle, download `files.jsonl.zip`. \n",
    "\n",
    "Then, upload `files.jsonl.zip` to the code space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Sure We Can Work With .py Files We Are Editing\n",
    "\n",
    "Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Things with Sentences\n",
    "\n",
    "So far our NLP experience has involved **tokens**. \n",
    "\n",
    "This week we will look at some things NLP can do with **sentences**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentences\n",
    "\n",
    "What is a sentence? The Oxford Dictionary of Linguistics says the term **sentence** is *Usually conceived, explicitly or implicitly, as the largest unit of grammar, or the largest unit over which a rule of grammar can operate*.\n",
    "\n",
    "What does this mean?\n",
    "\n",
    "1. A sentence can stand on its own.\n",
    "2. Parts of a sentence may also be able to stand on their own, in particular if a sentence is composed of **independent clauses**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Segmentation \n",
    "\n",
    "Let's use spaCy to get the sentences from a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "import spacy\n",
    "\n",
    "#import spacy_on_corpus\n",
    "import spacy_on_corpus\n",
    "\n",
    "# we will use a slightly larger spaCy model for this project\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# make a corpus from c*.txt\n",
    "my_corpus = spacy_on_corpus.build_corpus('c*.txt', {}, nlp)\n",
    "\n",
    "# extract a list of all the sentences in the corpus\n",
    "my_sentences = []\n",
    "for doc_id in my_corpus:\n",
    "    my_sentences.extend(list(my_corpus[doc_id]['doc'].sents))\n",
    "\n",
    "# print the sentences\n",
    "for sentence in my_sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect one sentence. What attributes does a sentence have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does a sentence have start, end and label attributes like an entity?\n",
    "for sentence in my_sentences:\n",
    "    print(sentence.start, sentence.end, sentence.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parses\n",
    "\n",
    "The structure of a token is described by its *morphology*.\n",
    "\n",
    "The structure of a sentence is described by its *syntax*. Spacy supports a particular model of syntax, called *dependency parsing*.\n",
    "\n",
    "Let's take a look at the dependency parse for some of these sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(my_sentences, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunks\n",
    "\n",
    "As you can tell, looking at the parses for *all* the sentences in a document can be overwhelming! Are there condensed representations of sentence structure, or phrase structure, that are easier to summarize or visualize? Yes!\n",
    "\n",
    "Let's look at how we can extract noun phrase **chunks** from spaCy sentence parses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of chunk counts\n",
    "my_chunks = {}\n",
    "\n",
    "# get all the chunks from the corpus\n",
    "for doc_id in my_corpus:\n",
    "    for chunk in my_corpus[doc_id]['doc'].noun_chunks:\n",
    "        if chunk not in my_chunks:\n",
    "            my_chunks[chunk.text] = 0\n",
    "        my_chunks[chunk.text] += 1\n",
    "\n",
    "# print the chunks\n",
    "print(my_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Similarities\n",
    "\n",
    "So far, we have only looked at the lexical (surface) and syntax (structure) of tokens, entities and sentences. What about the meanings?\n",
    "\n",
    "In NLP, we represent the *meaning in context* of a text using a **vector**, or list of numbers, learned from data. \n",
    "\n",
    "We can use the vectors for a pair of texts to calculate their **semantic** similarity (as opposed to just, say, the number of characters or words they have in common). We get a floating point number out when we do this.\n",
    "\n",
    "These vectors are quite large so they use a lot of memory. The spaCy `en_core_web_sm` model doesn't come with the vectors, but the `en_core_web_md` one does.\n",
    "\n",
    "Let's compare the vector similarity of all pairs of our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in my_sentences:\n",
    "    for sentence2 in my_sentences:\n",
    "        print(sentence, \"<->\", sentence2, sentence.similarity(sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the output similarities match your intuitions about the similarity or difference between the sentence pairs?\n",
    "\n",
    "Let's look a little more at this notion of vectors-as-meanings in NLP: https://jalammar.github.io/illustrated-word2vec/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Functions in spacy_on_corpus.py\n",
    "\n",
    "For this project, you will be extending your code in `spacy_on_corpus.py`. You will fill in the functions and test them in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `build_corpus`\n",
    "\n",
    "In the code cell below import `spacy_on_corpus` and run `build_corpus` on `test.jsonl`. Assign the result to `my_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_on_corpus\n",
    "\n",
    "my_corpus = spacy_on_corpus.build_corpus('test.jsonl', {}, nlp)\n",
    "\n",
    "print(my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `get_noun_chunk_counts`\n",
    "\n",
    "Complete the implementation of `get_noun_chunk_counts` in `spacy_on_corpus.py`. This will look a lot like `get_token_counts` or `get_entity_counts`. However, there are no tags to exclude.\n",
    "\n",
    "Then, in the code cell below import `spacy_on_corpus` and run `get_noun_chunk_counts` on `my_corpus`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_on_corpus\n",
    "\n",
    "spacy_on_corpus.get_noun_chunk_counts(my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer should look like:\n",
    "```\n",
    "[('Autumn', 1),\n",
    " ('Maine', 2),\n",
    " ('The leaves', 1),\n",
    " ('the trees', 1),\n",
    " ('the sky', 1),\n",
    " ('the iconic Maine rocks', 1),\n",
    " ('People', 2),\n",
    " ('leaf peeping', 1),\n",
    " ('apple picking', 1),\n",
    " ('a campfire', 1),\n",
    " ('the evenings', 1),\n",
    " ('Winter', 1),\n",
    " ('The snow', 1),\n",
    " ('all the deer ticks', 1),\n",
    " ('their skis', 1),\n",
    " ('snow shoes', 1),\n",
    " ('snowmobiles', 1),\n",
    " ('ice fishing', 1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `get_basic_statistics`\n",
    "\n",
    "Complete the implementation of `get_basic_statistics` in `spacy_on_corpus.py`. \n",
    "\n",
    "Then, in the code cell below import `spacy_on_corpus` and run `get_basic_statistics` on `my_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from spacy_on_corpus import get_basic_statistics\n",
    "\n",
    "# call get_basic_statistics on my_corpus\n",
    "get_basic_statistics(my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like:\n",
    "```\n",
    "Documents: 2\n",
    "\n",
    "Sentences: 7\n",
    "\n",
    "Tokens: 67\n",
    "\n",
    "Unique tokens: 50\n",
    "\n",
    "Entities: 5\n",
    "\n",
    "Unique entities: 3\n",
    "\n",
    "Chunks: 20\n",
    "\n",
    "Unique chunks: 18\n",
    "\n",
    "Publication year range: 2023-2023\n",
    "\n",
    "Page count year range: 1-1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `plot_token_frequencies`, `plot_entity_frequencies` and `plot_chunk_frequencies`\n",
    "\n",
    "Complete the implementation of these three functions in `spacy_on_corpus.py`. Make sure to exclude tokens that are 'uninformative'.\n",
    "\n",
    "Then, in the code cell below import `spacy_on_corpus` and run each of them on `my_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import spacy_on_corpus\n",
    "\n",
    "spacy_on_corpus.plot_token_frequencies(my_corpus)\n",
    "spacy_on_corpus.plot_entity_frequencies(my_corpus)\n",
    "spacy_on_corpus.plot_chunk_frequencies(my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting file `token_counts.png` should look like:\n",
    "\n",
    "![token_counts.png](answer_token_counts.png)\n",
    "\n",
    "The resulting file `entity_counts.png` should look like:\n",
    "\n",
    "![entity_counts.png](answer_entity_counts.png)\n",
    "\n",
    "The resulting file `chunk_counts.png` should look like:\n",
    "\n",
    "![chunk_counts.png](answer_chunk_counts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `plot_token_cloud`, `plot_entity_cloud` and `plot_chunk_cloud`\n",
    "\n",
    "Complete the implementation of these functions in `spacy_on_corpus.py`. Make sure to exclude tokens that are 'uninformative'.\n",
    "\n",
    "Then, in the code cell below import `spacy_on_corpus` and run each of these functions on `my_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import spacy_on_corpus\n",
    "# call plot_word_cloud on my_corpus\n",
    "spacy_on_corpus.plot_token_cloud(my_corpus)\n",
    "spacy_on_corpus.plot_entity_cloud(my_corpus)\n",
    "spacy_on_corpus.plot_chunk_cloud(my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting file `token_wordcloud.png` should look like:\n",
    "\n",
    "![token_wordcloud.png](answer_token_wordcloud.png)\n",
    "\n",
    "The resulting file `entity_wordcloud.png` should look like:\n",
    "\n",
    "![entity_wordcloud.png](answer_entity_wordcloud.png)\n",
    "\n",
    "The resulting file `chunk_wordcloud.png` should look like:\n",
    "\n",
    "![chunk_wordcloud.png](answer_chunk_wordcloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running `spacy_on_corpus.py` from the Terminal\n",
    "\n",
    "Complete the implementation of `main` in `spacy_on_corpus.py`. \n",
    "\n",
    "Now run this in the terminal:\n",
    "% `python spacy_on_corpus.py`\n",
    "\n",
    "Give it `files.jsonl.zip` as the pattern. Get all of 'statistics', 'count' and 'cloud' for 'corpus', and for any document id, 'statistics', 'markdown' and 'table' for that document..\n",
    "\n",
    "Copy the published statistics:\n",
    "```\n",
    "here\n",
    "```\n",
    "\n",
    "Insert the images generated when you run it.\n",
    "\n",
    "### Token count plot\n",
    "\n",
    "\n",
    "### Entity count plot\n",
    "\n",
    "### Chunk count plot\n",
    "\n",
    "\n",
    "### Token cloud\n",
    "\n",
    "### Entity cloud\n",
    "\n",
    "### Chunk cloud\n",
    "\n",
    "\n",
    "### Publication year plot\n",
    "\n",
    "\n",
    "### Page count plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer these questions.\n",
    "\n",
    "1. *What are some simple rules for sentence segmentation (splitting text into sentences)?*\n",
    "2. *What is an example sentence where your simple rules don't work?*\n",
    "3. *Why might I care what the syntax of a sentence is?*\n",
    "4. *Why might I care about the 'vector similarity' between two sentences?*\n",
    "5. *What is one difference between `en_core_web_sm` and `en_core_web_md`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
